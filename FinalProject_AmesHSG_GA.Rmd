---
title: "FinalProject_AmesHSG"
author: "Jaih, Katie, Andy, Anum"
date: "3/12/2021"
html_document:
    toc: true
    toc_depth: 4
    theme: cerulean
    highlight: textmate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package_load}
library(ggplot2) # graphics library
library(knitr)   # contains kable() function
library(tree)    # For the tree-fitting 'tree' function
library(rpart)   # For nicer tree fitting
library(partykit)  # For nicer tree plotting
library(randomForest) # For random forests and bagging
library(ggcorrplot)
library(ModelMetrics) #Functions for MAE, MSE, RMSE etc.

options(scipen = 4)  # Suppresses scientific notation


library(tidyverse)
library(GGally)
library(gridExtra) # for grid.arrange
```

## Section 1: Data Clean-Up and Exploration

<font color="#800000">
We are going to examine the housing data-set to pick out some key characteristics, trends and summaries for our model evaluations with the eventual goal of developing a sales price prediction model that can be used improve  price listings and clients advice on home improvement projects. The following section will aim to clean-up the data for our use and identify general trends in sales prices across time, neighborhood, and other key home characteristics. 
</font>

```{r}
#read in data
housing.raw <- read.csv("AmesHousing.csv")
```

```{r}
#summary(housing.raw)
glimpse(housing.raw)
#Checking the class of each variable
categories.housing <- data.frame(variabletype=sapply(housing.raw,class))
categories.housing
```
```{r}
numeric_variables <- table(categories.housing[categories.housing['variabletype'] == 'integer'])
numeric_variables
categorical_variables <- table(categories.housing[categories.housing['variabletype'] == 'character'])
categorical_variables
```


```{r}
#Checking the number of NA values in the data-set to evaluate clean-up
sum(is.na(housing.raw))
```

<font color="#800000">
On first glance, we see a few salient features within the dataset with 39 integer variables which include: 
-MS.Subclass
-Lot.Frontage
- Lot.Area
- Overall.Qual
- Overall.Cond
- Year.Built
- Year.Remod.Add
- Full.Bath				
- Half.Bath		
- Bedroom.AbvGr			
- Kitchen.AbvGr
- TotRms.AbvGrd
- Mo.Sold			
- Yr.Sold	
- SalePrice	

We see 43 character variables which include: 
- MS.Zoning
- Utilities
- Neighborhood
- Heating
- Central.Air
- Kitchen.Qual

The dataset also has `r sum(is.na(housing.raw))` NA values. However, in most cases this corresponds to the characteristic not being present in the house; therefore, we will proceed to change these 'NA' values to "None" where it makes sense to not lose any relevant data. e.g --> Pool.QC
</font>

```{r}
length.data.raw <- nrow(housing.raw)

#Replace empty strings with NA so that ensuing functions can mutate them
#Code example taken from: (https://stackoverflow.com/questions/51449243/how-to-replace-empty-string-with-na-in-r-dataframe)
housing.raw[housing.raw==""] <- NA

#Replace NA values from data adapted from (https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe)
housing.clean <- housing.raw %>% 
  mutate_if(is.character, ~replace(., is.na(.), "None")) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), 0))

#Identifying empty strings
housing.processed <- housing.clean %>%
  mutate(Bsmt.Cond = recode(Bsmt.Cond,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0)) %>%
  mutate(Bsmt.Exposure = factor(Bsmt.Exposure, levels = c("Gd","Av","Mn","No", "None"))) %>%
  mutate(Fireplace.Qu = recode(Fireplace.Qu,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Garage.Qual = recode(Garage.Qual,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Bsmt.Qual = recode(Bsmt.Qual,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(BsmtFin.Type.1 = factor(BsmtFin.Type.1, levels = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "None"))) %>%
  mutate(BsmtFin.Type.2 = factor(BsmtFin.Type.2, levels = c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf", "None"))) %>%
  mutate(Heating.QC = factor(Heating.QC, levels = c("Ex", "Gd", "TA", "Fa", "Po"))) %>%
  mutate(Kitchen.Qual = recode(Kitchen.Qual,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Exter.Qual = recode(Exter.Qual,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Exter.Cond = recode(Exter.Cond,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Functional = factor(Functional, levels = c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"))) %>%
  mutate(Garage.Finish = factor(Garage.Finish, levels = c("Fin", "RFn", "Unf", "None"))) %>%
  mutate(Pool.QC = recode(Pool.QC,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Heating.QC = recode(Heating.QC,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0))%>%
  mutate(Garage.Cond = recode(Garage.Cond,"Po" = 1, "Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5, .default = 0)) %>%
  mutate(Lot.Shape = factor(Lot.Shape, levels = c("Reg","IR1","IR2","IR3"))) %>%
  mutate(Land.Slope = factor(Land.Slope, levels = c("Gtl","Mod","Sev"))) %>%
  mutate(MS.Zoning = factor(MS.Zoning, levels = c("RL","RH","FV","RM","C (all)","I (all)","A (agr)"), ordered = FALSE)) %>%
  mutate(MS.SubClass = recode_factor(MS.SubClass, 
                                     '20' = "1-STORY 1946N",
                                     '30' = "1-STORY 1945O",
                                     '40' = "1-STORY UF.ATTIC.A",
                                     '45' = "1.5 STORY UF.A",
                                     '50' = "1.5-STORY F.ATTIC.A",
                                     '60' = "2-STORY 1946N",
                                     '70' = "2-STORY 1945O",
                                     '75' = "2.5-STORY A",
                                     '80' = "MULTILVL",
                                     '85' = "SPLIT FOYER",
                                     '90' = "DUPLEX.A",
                                     '120' = "1-STORY-PUD 1946N",
                                     '150' = "1.5-STORY-PUD.A",
                                     '160' = "2-STORY-PUD 1946N",
                                     '180' = "PUD-MULTILVL",
                                     '190' = "2 FAM CONV.A"))

housing.processed <- na.fail(subset(housing.processed, select = -c(ï..Order,PID)))

test <- which(housing.processed$Yr.Sold==2010)
train <- which(housing.processed$Yr.Sold<2010)

housing.train <- housing.processed[train,]
housing.test <- housing.processed[test,]

#terminology used is N for New, O for Old, A for All
        #20	1-STORY 1946 & NEWER ALL STYLES
        #30	1-STORY 1945 & OLDER
        #40	1-STORY W/FINISHED ATTIC ALL AGES
        #45	1-1/2 STORY - UNFINISHED ALL AGES
        #50	1-1/2 STORY FINISHED ALL AGES
        #60	2-STORY 1946 & NEWER
        #70	2-STORY 1945 & OLDER
        #75	2-1/2 STORY ALL AGES
        #80	SPLIT OR MULTI-LEVEL
        #85	SPLIT FOYER
        #90	DUPLEX - ALL STYLES AND AGES
       #120	1-STORY PUD (Planned Unit Development) - 1946 & NEWER
       #150	1-1/2 STORY PUD - ALL AGES
       #160	2-STORY PUD - 1946 & NEWER
       #180	PUD - MULTILEVEL - INCL SPLIT LEV/FOYER
       #190	2 FAMILY CONVERSION - ALL STYLES AND AGES
```



* only 198 entries have data for paved 

* Replaced all NA's with "NONE", so as to ensure that R counts it as a category

* Created levels for all data with c("Ex", "Gd", "TA", "Fa", "Po", "None"): BsmtCond, BsmtExposure, FireplaceQu, GarageQual, GarageCond, BsmtQual
- Corr:
- GarageQual, GarageCond

* Created levels for all data with  c("GLQ", "ALQ", "BLQ", "REC", "LwQ", "UNF", "None"): BsmtFinType1, BsmtFinType2

* ?Need to look at BsmtFinType1, BsmtFinType2 relationship, because it seems that unf/0 is for houses with one basement area

* Created levels for all data with c("Ex", "Gd", "TA", "Fa", "Po"): HeatingQC, KitchenQual, ExterQual, ExterCond

* Created levels for all data with  c("Typ", "Min1", "Min2", "Mod", "Maj1", "Maj2", "Sev", "Sal"): Functional

* Created levels for all data with  c("Fin", "RFn", "Unf", "None"): GarageFinish

* Created levels for all data with  c("Ex", "Gd", "TA", "Fa", "None"): PoolQC

* Created levels for all data with  c("Reg","IR1","IR2","IR3"): LotShape

* Created levels for all data with  c("Gtl","Mod","Sev"): LandSlope

* Created levels for all data with  c(10, 9, 8, 7, 6, 5, 4, 3, 2, 1): OverallQual, OverallCond

* Separate train and test

# Initial Data Plots and Summaries

<font color="#800000">
Now that we've done some cleaning; let's go on to explore our main dependent variable: SalePrice. 
</font>

```{r, warning = FALSE}
summary(housing.train$SalePrice)

# Histogram
qplot(housing.train$SalePrice, col = I("black"), fill = I("forest green")) + 
  xlab("Sales Price") + 
  ggtitle("Distribution of Sale Price in Data") 
```

<font color="#800000">
The data seems to have a slight right skewed distribution with a mean pricing of 184,386. When modeling this outcome, a strong argument can be made that the price should be log-transformed. A logarithmic price scale uses the percentage of change to plot data points, so, the scale prices are not positioned equidistantly. Furthermore, this will ensure that no houses would be predicted with negative sale prices and that errors in predicting expensive houses will not have an undue influence on the model. 
</font>

<font color="#800000">
For a better sense of the numeric variables that constitute area and key characteristics, we look at the histogram for a few. Given the importance of property size metrics, we quickly zoomed in on “GrLivArea”, “LotArea” and “GarageArea” as potential explanatory variables.
</font>

```{r}
par(mfrow = c(3, 3))
hist(housing.train$Lot.Frontage, breaks = 20, main = "Feet of Street Connected to Property", border="red", col="steelblue")
hist(housing.train$Lot.Area, breaks = 20, main = "Lot Size in Square Feet", border="red", col="steelblue")
hist(housing.train$Gr.Liv.Area, breaks = 20, main = "Above Ground Living Area", border="red", col="steelblue")
hist(housing.train$Pool.Area, breaks = 20, main = "Pool Area", border="red", col="steelblue")
hist(housing.train$Total.Bsmt.SF, breaks = 20, main = "Total Basement Square Feet", border="red", col="steelblue")
hist(housing.train$Year.Built, breaks = 20, main = "Year Built", border="red", col="steelblue")
hist(housing.train$Full.Bath, breaks = 20, main = "Full Bath", border="red", col="steelblue")
hist(housing.train$Bedroom.AbvGr, breaks = 20, main = "Bedroom Above Ground", border="red", col="steelblue")
hist(housing.train$Garage.Area, breaks = 20, main = "red", border="red", col="steelblue")
```

<font color="#800000">
Intuitively, neighbourhoods are a relevant predictor for Sale Price, so we proceed to make summary tables and boxplots to visualize this relationship. 
</font>

```{r, warning = FALSE}
neighborhood.eval <- housing.train %>%
  group_by(Neighborhood) %>%
  summarize(average.price = mean(SalePrice), 
            median.price = median(SalePrice), .groups = "keep",
            count = n())

kable(neighborhood.eval)
```

```{r}
#Neighborhood
ggplot(data = housing.train, aes(x =SalePrice , y = Neighborhood)) +
  geom_boxplot() + ggtitle("Neighborhood vs Sales Price")
```

<font color="#800000">
StoneBr, NridgeHt and NoRidge seem to be the more pricier neighborhoods. Our intuition was spot on: the sales price varies with different neighborhoods, so we should definitely include this in our model as a predictor.

Other characteristics that stands out is overall quality of the house and kitchen quality . We expect there to be a strong linear relationship between quality and prices. The poorer the quality, the lower the price. These can be strong predictors in our model.
</font>

```{r}
qual.plot <- ggplot(housing.train, aes(Overall.Qual,SalePrice)) + geom_jitter(alpha = 0.5, color = "blue")
qual.plot
```

```{r}
qual.plot2 <- ggplot(housing.train, aes(Kitchen.Qual,SalePrice)) + geom_jitter(alpha = 0.5, color = "blue")
qual.plot2
```

```{r}
#MSSubclass
ggplot(data = housing.train, aes(x = SalePrice, y = MS.SubClass)) +
  geom_boxplot()
```

```{r}
housing.train %>% ggplot(aes(x = Yr.Sold, y= SalePrice/1000)) + geom_jitter() + stat_smooth(method = "lm") + ggtitle("Housing Prices Between 2006-2010") + xlab("Year Sold") + ylab("Price in $1000")
```

```{r}
housing.train %>%
  group_by(Yr.Sold) %>%
  summarize(meanprice = mean(SalePrice))

```


<font color="#800000">
The dataset covers 2006-2010 which falls within the years of the Great Recession. The Great Recession was a period of marked general decline observed in national economies globally that began in December 2007 and ended in June 2009.  Home prices fell approximately 30 percent, on average, from their mid-2006 peak to mid-2009. Source URL: https://www.federalreservehistory.org/essays/great-recession-of-200709.

Given this fact, it is actually surprising that the dataset does not reflect a decrease in prices; but a stagnant rate across the five years.
</font>


<font color="#800000">
We also think it's meaningful to explore the relationship between price and year built. Generally, newer houses tend to be more expensive but there could elite neighborhoods with older houses that have a higher price. Let's see if this could be valuable for our model.
</font>


```{r}
year.plot <- ggplot(housing.train, aes(Year.Built,SalePrice)) + geom_jitter(alpha = 0.5, color = "blue")
year.plot
```

<font color="#800000">
Just as we expected, we see that newer houses tend to be more expensive in the dataset. However, there are a few outliers which exhibit the opposite: very old houses appear to increase in price. Perhaps these could be considered as "old century homes" built in years prior to 1920 which would no longer correspond to a price decrease with age. We would have to re-evalute this in our prediction model. 
</font>


```{r}
## Dig a bit deeper into any areas that show trends
#Sale Condition

ggplot(data = housing.train, aes(x = Sale.Condition, y = SalePrice)) +
  geom_boxplot()

#OverallCond

ggplot(data = housing.train, aes(x = Overall.Cond, y = SalePrice)) +
  geom_boxplot()

#OverallQual

ggplot(data = housing.train, aes(x = Overall.Qual, y = SalePrice)) +
  geom_boxplot()

#Functional
ggplot(data = housing.train, aes(x = Functional, y = SalePrice)) +
  geom_boxplot()

#Kitchen Quality

ggplot(data = housing.train, aes(x = Kitchen.Qual, y = SalePrice)) +
  geom_boxplot()

#BsmtCond

ggplot(data = housing.train, aes(x = Bsmt.Cond, y = SalePrice)) +
  geom_boxplot()

#Extercond

ggplot(data = housing.train, aes(x = Exter.Cond, y = SalePrice)) +
  geom_boxplot()

#HouseStyle
ggplot(data = housing.train, aes(x = House.Style, y = SalePrice)) +
  geom_boxplot()

#BldgType

ggplot(data = housing.train, aes(x = Bldg.Type, y = SalePrice)) +
  geom_boxplot()


#Sale Type

ggplot(data = housing.train, aes(x = Sale.Type, y = SalePrice)) +
  geom_boxplot()


```


<font color="#800000">
For the purpose of making out model leaner, we will explore variables that might exhibit high collinearity. Intuitively, it looks like there are some extra variables which may not be necessary. For example, our expectation is that Lot.Frontage and Lot.Area will be highly correlated. Similarly, Bedroom.AbvGr and TotRms.AbvGrd might follow a similar trend. We look further into the correlations between the numeric variables:
</font>


```{r}
# create a subset of variable names
myvars <- c("Lot.Frontage","Lot.Area", "Year.Built", "Year.Remod.Add", "Mas.Vnr.Area","Total.Bsmt.SF", "Bsmt.Unf.SF", "BsmtFin.SF.2","BsmtFin.SF.1", "X1st.Flr.SF","X2nd.Flr.SF", "Low.Qual.Fin.SF","Gr.Liv.Area", "Bsmt.Full.Bath","Bsmt.Half.Bath", "Full.Bath", "Half.Bath", "Bedroom.AbvGr","Kitchen.AbvGr", "Fireplaces", "TotRms.AbvGrd","Garage.Yr.Blt","Garage.Cars", "Garage.Area", "Wood.Deck.SF",	"Open.Porch.SF","Enclosed.Porch", "X3Ssn.Porch",	"Screen.Porch", "Pool.Area", "SalePrice", "Mo.Sold","Yr.Sold")

sub.house <- housing.train[myvars]

#Create a data frame for variables that are correlated 
corr.sub.house <- round(cor(sub.house),2)
corr.table <- as.data.frame(as.table(corr.sub.house))
names(corr.table)[1] <- "Variable 1"
names(corr.table)[2] <- "Variable 2"

#Find the variables which are strongly correlated as per rule of 0.6
corr.subset <- subset(corr.table,  Freq > 0.6 | Freq < -0.6) 
high.corr <- data.frame(corr.subset)
high.corr <- high.corr[order(high.corr$Freq),]
high.corr <- high.corr %>%
  filter(Freq < 1)
name1 <- as.character(high.corr$Variable.1)
name2 <- as.character(high.corr$Variable.2)
hi.cor.vars <- unique(append(name1, name2))
hi.cor.vars
```

```{r}
categories.housing
```


<font color="#800000">
Based on intuition, we split the variables into sub-categories to explore collinearity. 
</font>

```{r}
#Features
hsg.features <- c("Utilities", "Bldg.Type","House.Style","Roof.Style",
                  "Roof.Matl","Exterior.1st","Exterior.2nd","Foundation",
                  "Bsmt.Unf.SF","Total.Bsmt.SF","Heating","Heating.QC",
                  "Central.Air","Electrical","Fireplaces","Garage.Type",
                  "Garage.Finish","Garage.Area","Paved.Drive","Misc.Feature",
                  "Misc.Val")

#Quality
hsg.quality <- c("Overall.Qual","Overall.Cond","Exter.Qual","Exter.Cond",
                 "Bsmt.Cond","Bsmt.Qual","Bsmt.Exposure","BsmtFin.Type.1",
                 "BsmtFin.Type.2","Heating.QC","Fireplace.Qu","Garage.Finish",
                 "Garage.Qual","Garage.Cond","Fence","Misc.Val","Pool.QC",
                 "Sale.Condition")

#Sales Criteria
sales.crit <- c("MS.SubClass", "MS.Zoning", "Year.Built", "Year.Remod.Add",
                "Mo.Sold", "Yr.Sold", "Sale.Type")

# Use the following characterstics to make correlation plots and pick out the most relevant 
#Geo-Spatial Characteristics
geo.char <- c("Lot.Area","Lot.Frontage","Mas.Vnr.Area","X1st.Flr.SF","X2nd.Flr.SF","Garage.Area","Wood.Deck.SF",
              "Open.Porch.SF","Pool.Area", "Low.Qual.Fin.SF","Gr.Liv.Area")
  
  
ggpairs(data = housing.train[,geo.char[1:5]])

#Basement/Bath Redundancies
bb.crit <- c("BsmtFin.SF.1", "BsmtFin.SF.2", "Bsmt.Unf.SF", "Total.Bsmt.SF", 
             "Bsmt.Full.Bath", "Bsmt.Half.Bath", "Full.Bath","Half.Bath")

#Above Ground
ag.crit <- c("Bedroom.AbvGr", "Kitchen.AbvGr", "TotRms.AbvGrd", "Gr.Liv.Area")

#Deck/Porch
dp.crit <- c("Wood.Deck.SF", "Open.Porch.SF", "Enclosed.Porch","X3Ssn.Porch","Screen.Porch")


sub.house.geo <- housing.train[geo.char]
corr.sub.house.geo <- round(cor(sub.house.geo),2)

sub.house.bb.crit <- housing.train[bb.crit]
corr.sub.house.bb <- round(cor(sub.house.bb.crit),2)

sub.house.ag.crit <- housing.train[ag.crit]
corr.sub.house.ag <- round(cor(sub.house.ag.crit),2)

sub.house.dp.crit <- housing.train[dp.crit]
corr.sub.house.dp <- round(cor(sub.house.dp.crit),2)

```

```{r, fig.width = 15}
#highly correlated variables
sub.house.high <- housing.train[hi.cor.vars]
corr.sub.house.high <- round(cor(sub.house.high),2)


#Use ggcorrplot to graph correlation. 
ggcorrplot(corr.sub.house.high, hc.order = TRUE, outline.color = 'white', lab = TRUE, title = "Highly Correlated Variables in Housing Dataset", legend.title = "Correlation")
```


```{r, fig.width = 15}
#Use ggcorrplot to graph correlation. 
ggcorrplot(corr.sub.house.geo, hc.order = TRUE, outline.color = 'white', lab = TRUE, title = "Correlation between Geo-Spatial Characteristics in Housing Dataset", legend.title = "Correlation")
```

```{r, fig.width = 15}
#Use ggcorrplot to graph correlation. 
ggcorrplot(corr.sub.house.bb, hc.order = TRUE, outline.color = 'white', lab = TRUE, title = "Correlation between Basement-Bath Characteristics in Housing Dataset", legend.title = "Correlation")
```
```{r, fig.width = 15}
#Use ggcorrplot to graph correlation. 
ggcorrplot(corr.sub.house.ag, hc.order = TRUE, outline.color = 'white', lab = TRUE, title = "Correlation between Basement-Bath Characteristics in Housing Dataset", legend.title = "Correlation")
```
```{r, fig.width = 15}
#Use ggcorrplot to graph correlation. 
ggcorrplot(corr.sub.house.dp, hc.order = TRUE, outline.color = 'white', lab = TRUE, title = "Correlation between Basement-Bath Characteristics in Housing Dataset", legend.title = "Correlation")
```

# Functions to Evaluate models
```{r}
calcRelErr <- function(preds, actual) {
  (preds - actual)/actual
}
```

# Inital Linear Regressions 

<font color="#800000">
As an initial simple model we will consider a few variables to be important determinants of the price of a property:

- Overall.Qual: The quality rating (from 1 to 10).
- Kitchen.Qual
- Neighborhood: Neighborhoods vary across location as per our EDA.
- MS.SubClass
- MS.Zoning.
- Lot.Area
- Year.Built 
- Total.Bsmt.SF
- Gr.Liv.Area
- Open.Porch.SF
- Pool.Area
</font>

```{r}
lm.fit.1 <- lm(SalePrice ~
                 Overall.Qual +
                 Kitchen.Qual +
                 Neighborhood +
                 MS.SubClass +
                 MS.Zoning +
                 Lot.Area +
                 Year.Built +
                 Total.Bsmt.SF + 
                 Gr.Liv.Area +
                 Open.Porch.SF +
                 Pool.Area,
               data = housing.train)
summary(lm.fit.1)


lm.preds = predict(lm.fit.1, newdata=housing.test)
lm.resid = housing.test$SalePrice - lm.preds

#Residual Plot for Linear Model 1
ggplot(data=data.frame(lm.preds, lm.resid), aes(lm.preds,lm.resid)) +
  geom_point(size=1,colour="lightblue") +
  geom_hline(yintercept = mean(lm.resid), color="red")+
  theme_bw() 


lm.MAE <- mae(actual=housing.test$SalePrice, predicted=lm.preds, lm.fit.1)
lm.RMSE <- rmse(actual=housing.test$SalePrice, predicted=lm.preds, lm.fit.1)

paste("MAE: ", lm.MAE )
paste("RMSE: ", lm.RMSE)
paste("Mean RelErr: ", mean(calcRelErr(preds=lm.preds, housing.test$SalePrice)))


```




Let's see if subset selection produces similar results to our own intuitive selection.

```{r}
library(leaps)
# Categorical Vars - MSSubClass, MSZoning, Street, Alley, Lot Shape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofStyle, RoofMat1, Exterior1st, Exterior2nd, MasVnrType, MasVnrArea, ExterQual,ExterCond, Foundation, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFinType2, Heating, HeatingQC, CentralAir, Electrical, KitchenQual, TotRmsAbvGrd, Functional, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageQual, GaraggeCond, PavedDrive, PoolQC, Fence, MiscFeature, MoSold, YrSold, SaleType, SaleCondition

# Continuous Variables - LotFrontage, LotArea, OverallQual,OverallCond, YearBuilt, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal 
```

```{r}
nums <- unlist(lapply(housing.train, is.numeric))
num.housing.train <- housing.train[,nums]
```

```{r}

regfit.fwd <- regsubsets(SalePrice~., data = num.housing.train, nbest = 1, nvmax = NULL, method = "forward", really.big=TRUE)

```

```{r}
regfit.summary <- summary(regfit.fwd)

num_variables<-seq(1,length(regfit.summary$rss))

plot_RSS<-ggplot(data = data.frame(regfit.summary$rss),
                 aes(x=num_variables,y=regfit.summary$rss))+
  geom_line()+
  geom_point(x=which.min(regfit.summary$rss),
             y=min(regfit.summary$rss),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("RSS")+
  theme_bw()

plot_R_sq<-ggplot(data = data.frame(regfit.summary$rsq),
                 aes(x=num_variables,y=regfit.summary$rsq))+
  geom_line()+
  geom_point(x=which.max(regfit.summary$rsq),
             y=max(regfit.summary$rsq),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("R-sq")+
  theme_bw()

plot_BIC<-ggplot(data = data.frame(regfit.summary$bic),
                 aes(x=num_variables,y=regfit.summary$bic))+
  geom_line()+
  geom_point(x=which.min(regfit.summary$bic),
             y=min(regfit.summary$bic),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("BIC")+
  theme_bw()

plot_AIC<-ggplot(data = data.frame(regfit.summary$cp),
                 aes(x=num_variables,y=regfit.summary$cp))+
  geom_line()+
  geom_point(x=which.min(regfit.summary$cp),
             y=min(regfit.summary$cp),aes(color="red"),
             show.legend = FALSE)+
  xlab("# Variables")+
  ylab("AIC")+
  theme_bw()


grid.arrange(plot_RSS, plot_R_sq,plot_AIC,plot_BIC, ncol=2,nrow=2)

```
```{r}
#variables for lowest BIC
coef(regfit.fwd, which.min(regfit.summary$bic))
```

# Lasso Model

```{r}
library(glmnet)
#Lasso 
x <- model.matrix(SalePrice~.,housing.processed)[,-1]
y <- housing.processed$SalePrice

#Create Lasso Models for CV
grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

#Run CV
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)

#Find BestLam from CV
bestlam=cv.out$lambda.min


lasso.coef = predict(lasso.mod,type="coefficients",s=bestlam)[1:40,]
kable(lasso.coef[lasso.coef!=0])
length(lasso.coef[lasso.coef!=0])

lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
lasso.resid = housing.test$SalePrice - lasso.pred

#Residual Plot for Linear Model 1
ggplot(data=data.frame(lasso.pred, lasso.resid), aes(lasso.pred,lasso.resid)) +
  geom_point(size=1,colour="lightblue") +
  geom_hline(yintercept = mean(lasso.resid), color="red")+
  theme_bw() 



paste("MAE: ", mae(predicted=lasso.pred, actual=y[test], lasso.mod))
paste("RMSE: ", rmse(predicted=lasso.pred, actual=y[test], lasso.mod))
paste("Mean RelErr: ", mean(calcRelErr(preds=lasso.pred, y[test])))
```


# Random Forests
```{r}
set.seed(1)
rf <- randomForest(SalePrice ~ .,data=housing.train, mtry=25, importance=TRUE)
rf

varImpPlot(rf,n.var = 20)

rf.preds <- predict(rf, newdata = housing.test)

rf.resid = housing.test$SalePrice - rf.preds

#Residual Plot for Linear Model 1
ggplot(data=data.frame(rf.preds, rf.resid), aes(rf.preds,rf.resid)) +
  geom_point(size=1,colour="lightblue") +
  geom_hline(yintercept = mean(rf.resid), color="red")+
  theme_bw() 

lm.MAE <- mae(actual=housing.test$SalePrice, predicted=rf.preds, rf)
lm.RMSE <- rmse(actual=housing.test$SalePrice, predicted=rf.preds, rf)

paste("MAE: ",lm.MAE)
paste("RMSE: ",lm.RMSE)
paste("Mean RelErr: ", mean(calcRelErr(preds=rf.preds, actual=housing.test$SalePrice)))


```
# Random Forest Variables
```{r}
#I think this is redundant information
rf.imp.plot <- varImpPlot(rf)
imp.vars <- as.character(rownames(rf.imp.plot)[order(rf.imp.plot, decreasing = TRUE)])
imp.vars <- imp.vars[c(80:158)]
imp.vars
rf$importance
```



```{r}
#Out-of-bag MSE curve
plot(rf)
```

This plot shows that the Out-of-Bag (OOB) test error plateaus at around 400 trees. We will run a test to see which mtry value gives the lowest OOB test error.


```{r}
mtry <- tuneRF(housing.train[-80], housing.train$SalePrice, ntreeTry = 500,
               stepFactor = 1.5, improve = 0.01, trace = TRUE, plot = TRUE)
best.m <- mtry[mtry[,2] == min(mtry[,2]), 1]

print(mtry)
print(best.m)
```

According to the tuning model, the random forest model that results in the lowest Out-of-Bag error estimates uses an mtry value of 39. The next three lowest OOB Errors come with mtry values of 18, 26, and 58. Note, we originally ran our model with an mtry value of 25. 

```{r}
#set.seed(1)
#rf <- randomForest(SalePrice ~ .,data=housing.train, mtry=25, importance=TRUE)
#rf

#varImpPlot(rf,n.var = 20)

#rf.preds <- predict(rf, newdata = housing.test)
#lm.MAE <- mae(actual=housing.test$SalePrice, predicted=rf.preds, rf)
#lm.RMSE <- rmse(actual=housing.test$SalePrice, predicted=rf.preds, rf)

#paste("MAE: ",lm.MAE)
#paste("RMSE: ",lm.RMSE)
#paste("Mean RelErr: ", mean(calcRelErr(preds=rf.preds, actual=housing.test$SalePrice)))

```

# GAM Model

```{r}
library(gam)
library(plyr)
library(dplyr)
```


```{r}
#Poly, cubic, natural, and smoothing
polyTestErr <- function(dat, train, d) {
  poly.fit <- lm(y ~ poly(x, degree = d), data = dat, subset = train)
  preds <- predict(poly.fit, dat)[-train]
  mean((dat$y[-train] - preds)^2)
}

cubicSplineTestErr <- function(dat, train, df) {
  if(df >= 3) {
    spline.fit <- lm(y ~ bs(x, df = df), data = dat, subset = train)
    preds <- predict(spline.fit, dat)[-train]
    mean((dat$y[-train] - preds)^2)
  } else {
    NA
  }
}

naturalSplineTestErr <- function(dat, train, df) {
  if(df >= 1) {
    spline.fit <- lm(y ~ ns(x, df = df), data = dat, subset = train)
    preds <- predict(spline.fit, dat)[-train]
    mean((dat$y[-train] - preds)^2)
  } else {
    NA
  }
}

smoothSplineTestErr <- function(dat, train, df) {
  if(df > 1) {
    spline.fit <- with(dat, smooth.spline(x[train], y[train], df = df))
    preds <- predict(spline.fit, dat$x)$y[-train]
    mean((dat$y[-train] - preds)^2)
  } else {
    NA
  }
}

```


```{r}
smoothCV <- function(x, y, K = 10, df.min = 1, df.max = 10) {
  dat <- data.frame(x = x, y = y)
  n <- length(y) # number of observations
  
  num.methods <- 4
  method.names <- c("poly", "cubic.spline", "natural.spline", "smoothing.spline")
  err.out <- data.frame(df = rep(df.min:df.max, each = num.methods),
                        method = rep(method.names, df.max - df.min + 1))
  set.seed(1)
  # Get a random permutation of the indexes
  random.perm <- sample(n)
  # break points for the folds.  If n is not evenly divisible by K,
  # these may not be of exactly the same size.
  fold.breaks <- round(seq(1,n+1, length.out = K + 1))
  fold.start <- fold.breaks[1:K]
  fold.end <- fold.breaks[2:(K+1)] - 1
  fold.end[K] <- n # Fix the last endoint to equal n
  fold.size <- fold.end - fold.start + 1 # num obs in each fold
  
  cv.err <- NULL
  fold.err <- matrix(0, nrow = K, ncol = 4)
  colnames(fold.err) <- c("poly", "cubic.spline", "natural.spline", "smoothing.spline")
  # Outer loop: Loop over degrees of freedom
  # Inner loop: Iterate over the K folds
  for(df in df.min:df.max) {
    for(k in 1:K) {
      test.idx <- fold.start[k]:fold.end[k]
      train <- random.perm[-test.idx]
      
      # Calculate test error for the four models
      poly.err <- polyTestErr(dat, train = train, d = df)
      cubic.spline.err <- cubicSplineTestErr(dat, train = train, df = df)
      natural.spline.err <- naturalSplineTestErr(dat, train = train, df = df)
      smooth.spline.err <- smoothSplineTestErr(dat, train = train, df = df)
      
      # Store results for this fold
      fold.err[k,] <- c(poly.err, cubic.spline.err, natural.spline.err, smooth.spline.err)
#       print(fold.err[k,])
    }
    # Perform weighted averaging to calculate CV error estimate
    # MSE estimates from each fold are weighted by the size of the fold
    # If all folds are the same size, this is the same thing as the unweighted
    # average of all of the MSE's
    err.ave <- colSums(sweep(fold.err, MARGIN = 1, fold.size, FUN = "*") / n)
    cv.err <- c(cv.err, err.ave)
  }
  err.out$cv.error <- cv.err
  err.out
}
```

```{r}
# This plotting approach has a facet option which allows the user to show
# three separate plots instead of overlaying the curves
# If y.scale.factor is non-null, the range of the 
# y-axis for the plot is restricted to y.min to y.min*y.scale.factor
plot.smoothCV <- function(smoothcv.err, K, title.text = "", facet = FALSE,
                          y.scale.factor = NULL) {

  # Convert the method names
  dat <- transform(smoothcv.err, 
                   method = mapvalues(method,
                                      c("poly", "cubic.spline", "natural.spline", "smoothing.spline"),
                                      c("Polynomial", "Cubic Spline", "Natural Spline", "Smoothing Spline")
                                      )
                   )
  
  # Set axes labels
  x.text <- "Degrees of Freedom"
  y.text <- paste0(K, "-fold CV Error")
  
  # The ggplot "setting": data, axes, and color by method
  p <- ggplot(data = dat, aes(x = df, y = cv.error, colour = method)) 
  
  # Overlay with line plots, data points, axes labels, and graph title
  p <- p + geom_line() + geom_point() + xlab(x.text) + ylab(y.text) +
          ggtitle(title.text)
  
  # Adjust the y axis range if y.scale.factor is specified
  if(!is.null(y.scale.factor)) {
    min.err <- min(dat$cv.error, na.rm = TRUE)
    p <- p + ylim(min.err, y.scale.factor * min.err)
  }
  
  # Show a separate plot per method if facet=TRUE
  if(!facet) {
    print(p)
  } else {
    print(p + facet_wrap("method"))
  }
}
```

# Finding models with lowest CV Error

<font color="#800000">
As per our earlier analyses, the following variables seem to be key components in evaluating a GAM model:

Variables to use: Gr.Liv.Area, Overall.Qual, X1st.Flr.SF, MS.SubClass, X2nd.Flr.SF, Total.Bsmt.SF, MS.Zoning, Full.Bath, Kitchen.Qual, Neighborhood, and Central.Air
</font>



```{r}
gr.area.cv <- smoothCV(x = housing.train$Gr.Liv.Area,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 10)

plot.smoothCV(gr.area.cv, K = 10, title.text = "CV Error: SalePrice ~ Gr.Liv.Area",
              y.scale.factor = 1.1)
```
<font color="#800000">
We will use a cubic spline of 4 with the variable Gr.Liv.Area. This has the lowest 10-fold CV error. 
</font>



```{r}
qual.cv <- smoothCV(x = housing.train$Overall.Qual,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 9)

plot.smoothCV(qual.cv, K = 10, title.text = "CV Error: SalePrice ~ Overall.Qual")
```

<font color="#800000">
Our CV error model suggests we use a natural spline with df = 3. Our earlier analysis showed a linear relationship between price and overall quality and it is a key predictor for our model. When finalizing we will run estimates with both to see which adds more value to our model with less error. 
</font>



```{r}
X1st.flr.cv <- smoothCV(x = housing.train$X1st.Flr.SF,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 10)

plot.smoothCV(X1st.flr.cv, K = 10, title.text = "CV Error: SalePrice ~ 1st.Flr.SF",
              y.scale.factor = 1.1)
```

<font color="#800000">
We will use a polynomial with degree 6 for 1st.Flr.SF. 
</font>


```{r}
X2nd.flr.cv <- smoothCV(x = housing.train$X2nd.Flr.SF,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 10)

plot.smoothCV(X2nd.flr.cv, K = 10, title.text = "CV Error: SalePrice ~ 2nd.Flr.SF",
              y.scale.factor = 1.1)
```

<font color="#800000">
We will use a polynomial with degree 3 for 2nd.Flr.SF
</font>



```{r}
basement.cv <- smoothCV(x = housing.train$Bsmt.Qual,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 5)

plot.smoothCV(basement.cv, K = 10, title.text = "CV Error: SalePrice ~ Bsmt.Qual",
              y.scale.factor = 1.1)
```


<font color="#800000">
We will use a smoothing spline with degree 5 for Bsmt.Qual
</font>


```{r}
bath.cv <- smoothCV(x = housing.train$Full.Bath,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 4)

plot.smoothCV(bath.cv, K = 10, title.text = "CV Error: SalePrice ~ Full.Bath",
              y.scale.factor = 1.1)
```

<font color="#800000">
We will use a polynomial with df = 3 
</font>


```{r}
kitchen.cv <- smoothCV(x = housing.train$Kitchen.Qual,
         y = housing.train$SalePrice,
         K = 10,
         df.min = 1,
         df.max = 3)

plot.smoothCV(kitchen.cv, K = 10, title.text = "CV Error: SalePrice ~ Kitchen.Qual")
```

<font color="#800000">
Our CV error model suggests we use a polynomial with df = 2. As with overall quality, kitchen quality is a salient predictor and our initial summaries corroborate the finding of a linear relationship between price and kitchen quality; we will go for a linear fit. 
</font>

```{r}


gam.fit <- gam(SalePrice ~ bs(Gr.Liv.Area, 4) + ns(Overall.Qual,3) + s(X1st.Flr.SF, 10) + MS.SubClass +
                 poly(X2nd.Flr.SF, 3) + ns(Bsmt.Qual, 3) + MS.Zoning + ns(Full.Bath, 4) + 
                 poly(Kitchen.Qual,2) + Neighborhood + Utilities + Lot.Area, data = housing.train)
summary(gam.fit)

gam.deviance <- 1 - gam.fit$deviance/gam.fit$null.deviance
gam.deviance
```

```{r}
gam.preds <- predict(gam.fit, newdata = housing.test)
gam.MAE <- mae(actual=housing.test$SalePrice, predicted=gam.preds, gam.fit)
gam.RMSE <- rmse(actual=housing.test$SalePrice, predicted=gam.preds, gam.fit)

gam.resid = housing.test$SalePrice - gam.preds

#Residual Plot for Linear Model 1
ggplot(data=data.frame(gam.preds, gam.resid), aes(gam.preds,gam.resid)) +
  geom_point(size=1,colour="lightblue") +
  geom_hline(yintercept = mean(gam.resid), color="red")+
  theme_bw() 

paste("MAE: ",gam.MAE)
paste("RMSE: ",gam.RMSE)
paste("Mean RelErr: ", mean(calcRelErr(preds=gam.preds, actual=housing.test$SalePrice)))

```


<font color="#800000">
Our GAM model accounts for roughly 88.9% of the deviance explained. The percent deviance explained is the Generalized Additive Model analog of R-squared. It is exactly equal to the R-squared for regression models that can be fit with both the gam and lm functions. After cutting down to relevant variables, we attempt to recreate a multiple linear regression model. When predicting our model, we used natural spline and linear model for overall quality. However, the prediction error was less with the natural spline. To see if a multiple linear model does better, we fit the same variables on multiple regression. 
</font>

```{r}
#run a multiple linear regression of SalePrice on the same model
lm.fit.2 <- lm(SalePrice ~ Gr.Liv.Area + Overall.Qual + X1st.Flr.SF + MS.SubClass + X2nd.Flr.SF + Total.Bsmt.SF + MS.Zoning + Full.Bath + Kitchen.Qual + Neighborhood + Central.Air , data = housing.train)

summary(lm.fit.2)
```


```{r}
lm.preds.2 = predict(lm.fit.2, newdata=housing.test)
lm.MAE.2 <- mae(actual=housing.test$SalePrice, predicted=lm.preds.2, lm.fit.2)
lm.RMSE.2 <- rmse(actual=housing.test$SalePrice, predicted=lm.preds.2, lm.fit.2)

paste("MAE: ", lm.MAE.2 )
paste("RMSE: ", lm.RMSE.2)
paste("Mean RelErr: ", mean(calcRelErr(preds=lm.preds.2, housing.test$SalePrice)))
```


# Home Improvement - Sale Price Calculator

id: the row index for the home 
basement: the level upgrade requested ("GLQ", "ALQ", "BLQ", "Rec", "LwQ")
full.bath: the # of full bathrooms to add
half.bath: the # of half bathrooms to add
kitchen: the level upgrade requested (Fa" = 2, "TA" = 3, "Gd" = 4, "Ex" = 5)
AC: "Yes" if adding central air conditioning

```{r}
possible.homes <- housing.test
possible.homes$predicted <- rf.preds

possible.homes1k <- possible.homes %>%
  filter(abs(predicted - SalePrice) <= 1000)

possible.homes500 <- possible.homes %>%
  filter(abs(predicted - SalePrice) <= 500)

possible.homes100 <- possible.homes %>%
  filter(abs(predicted - SalePrice) <= 100)

possible.homes5k <- possible.homes %>%
  filter(abs(predicted - SalePrice) <= 5000)


possible.homes <- possible.homes %>%
  filter(abs(predicted - SalePrice) <= 2000)

homes <- housing.test
homes$predicted <- gam.preds

homes2k <- homes %>%
  filter(abs(predicted - SalePrice) <= 2000)

```


```{r}
price.calculator <- function(id, basement = 0, full.bath = "null", half.bath = "null", kitchen = 0, AC = 0) {
  current.price <- possible.homes$SalePrice[id]
  pred.price <- predict(rf, newdata = possible.homes[id,])
  diff <- abs(current.price - pred.price)
  
  df.homes <- possible.homes
  
  if(!basement == 0) {
    df.homes$BsmtFin.Type.1[id] <- basement
  }
  if(!full.bath == "null") {
    df.homes$Full.Bath[id] <- full.bath
  }
  if(!half.bath == "null") {
    df.homes$Half.Bath[id] <- half.bath
  }
  if(!kitchen == 0) {
    df.homes$Kitchen.Qual[id] <- kitchen
  }
  if(!AC == 0) {
    df.homes$Central.Air[id] <- AC
  }
  
  updated.price <- predict(rf, newdata = df.homes[id])
  
  output <- data.frame(Current_Price = current.price,
                       Current_Predicted_Price = pred.price,
                       Updated_Price = updated.price)
  print(output)
  
}
```


```{r}
#price.calculator(id = 10, full.bath = 3)
```

```{r}
price.calculator2 <- function(id, basement = "no", full.bath = "no", half.bath = "no", kitchen = "no", AC = "no") {
  current.price <- possible.homes$SalePrice[id]
  pred.price <- predict(rf, newdata = possible.homes[id,])
  diff <- abs(current.price - pred.price)
  
  df.basement <- possible.homes
  df.fullbath <- possible.homes
  df.halfbath <- possible.homes
  df.kitchen <- possible.homes
  df.AC <- possible.homes
  
  basement.lvl <- c("GLQ", "ALQ", "BLQ", "Rec", "LwQ", "Unf")
  kitchen.lvl <- c(1, 2, 3, 4, 5)
  
  basement.plot <- c()
  fullbath.plot <- c()
  halfbath.plot <- c()
  kitchen.plot <- c()
  AC.plot <- c()
  
  if(basement == "yes") {
    for(i in basement.lvl) {
      df.basement$BsmtFin.Type.1[id] <- i
      basement.plot[i] <- predict(rf, newdata = df.basement[id,])
    }
    df1 <- data.frame(rating = basement.lvl,
                    Sale_Price = basement.plot)
    print(qplot(x = rating, y = Sale_Price, data = df1) + geom_point() +
      geom_point(aes(x = possible.homes$BsmtFin.Type.1[id], y = possible.homes$predicted[id]), col = I("red")))
  }
  
  if(full.bath == "yes") {
    for(i in 1:5) {
      df.fullbath$Full.Bath[id] <- i
      fullbath.plot[i] <- predict(rf, newdata = df.fullbath[id,])
    }
    df2 <- data.frame(number = c(1:5),
                    Sale_Price = fullbath.plot)
    print(qplot(x = number, y = Sale_Price, data = df2) + geom_point() +
      geom_point(aes(x = possible.homes$Full.Bath[id], y = possible.homes$predicted[id]), col = I("red")))
  }  
  
  if(half.bath == "yes") {
    for(i in 1:3) {
      df.halfbath$Half.Bath[id] <- i
      halfbath.plot[i] <- predict(rf, newdata = df.halfbath[id,])
    }
    df3 <- data.frame(number = c(1:3),
                    Sale_Price = halfbath.plot)
    print(qplot(x = number, y = Sale_Price, data = df3) + geom_point() +
      geom_point(aes(x = possible.homes$Half.Bath[id], y = possible.homes$predicted[id]), col = I("red")))
  } 
   
  if(kitchen == "yes") {
    for(i in kitchen.lvl) {
      df.kitchen$Kitchen.Qual[id] <- i
      kitchen.plot[i] <- predict(rf, newdata = df.kitchen[id,])
    }
    df4 <- data.frame(rating = kitchen.lvl,
                    Sale_Price = kitchen.plot)
    print(qplot(x = rating, y = Sale_Price, data = df4) + geom_point() +
      geom_point(aes(x = possible.homes$Kitchen.Qual[id], y = possible.homes$predicted[id]), col = I("red")))
  }
  
  if(AC == "yes") {
    for(i in c("no", "yes")) {
      df.AC$Central.Air[id] <- i
      AC.plot[i] <- predict(rf, newdata = df.AC[id,])
    }
    df5 <- data.frame(Have_AC = c("no", "yes"),
                    Sale_Price = AC.plot)
    print(qplot(x = Have_AC, y = Sale_Price, data = df5) + geom_point() +
      geom_point(aes(x = possible.homes$Central.Air[id], y = possible.homes$predicted[id]), col = I("red")))
  }
  
  
}
```

```{r}
price.calculator2(id = 10, basement = "yes", full.bath = "yes", half.bath = "yes", kitchen = "yes", AC = "yes")
```


```{r}
##Subset Data into comparison homes from the 2010 data
# - Potentially take user input into what types of homes they are interested in (potentially a subset)

##Select Subset of features and output plots that show bar charts of average sale price as a function of the upgrade feature
# - For qualitative data use levels or categorize levels (general 1-3, major 5-7, extreme 8-10)
# - This does not need to be feature for every node within the data, but what we feel is most salient (i.e. N >> some threshold)
# - Mark plots with average expenditures to show level of commitment and expenditure

## Based on the user input, output plots that show housing value as a function of of the upgrade feature for the specific 2010 house or subset of the houses

## Select items with MAX housing price for that type of house and then output the suggestion (even if slightly unreasonable)

```



```{r}
possible.homes %>%
  select(BsmtFin.Type.1, Full.Bath, Half.Bath, Kitchen.Qual, Central.Air, SalePrice, predicted)
```











